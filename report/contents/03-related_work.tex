\section{Related Work}

\subsubsection{Scene Graph Generation (SGG)}
This is a method for parsing images or videos into an intermediate graph representation for downstream tasks. Traditional SGG methods assume a closed-vocabulary settings, where both object categories and predicate labels are predefined and fixed during training. Most closed-vocab approaches typically adopt a two-stage framework \cite{li2024survey},
which have a freeze object detector and focus on training the relation prediciton. However, these methods suffer from the long-tail distribution problem, in which rare relationships are poorly captured \cite{he2022towards}. Real-life scenarios are open and contain numerous unknown relationships, which limits the applicability of closed-set-based methods \cite{min2025interactive}. Previous proposals to these challenges are Weakly Supervised Scene Graph Generation (WS-SGG) \cite{zareian2020weakly} and Zero-shot Scene Graph Generation (Zs-SGG) \cite{lu2016visual}. Weakly supervised SGG reduces annotation dependency by leveraging image-level or partial supervision, while Zs-SGG is dedicated to predicting relationships that do not appear in the training set, the objects are still restricted to seen categories \cite{he2022towards}. Recently, Open-vocabulary Scene Graph Generation (Ov-SGG) methods have introduced approaches for predicting novel visual relationships involving unseen objects \cite{he2022towards}.

\subsubsection{Open-vocabulary Scene Graph Generation (Ov-SGG)}
This task aims to recognize and predict relationships involving unseen object categories or relations that are not present in the training data. Recent approaches leverage large-scale pretrained models and external knowledge sources to address this challenge \cite{he2022towards}.
Ov-SGG methods can be categorized into two main approaches\cite{li2025interaction}: (1) VLM-based methods and (2) MLLM-based methods. \textbf{MLLM-based} approaches integrate large language models to reason over detected objects and generate plausible relational triplets. Representative works such as LLaVA-SpaceSGG \cite{xu2025llava} adapt the LLaVA MLLM architecture by fine-tuning it on spatially aware instruction data.
This approach enables the model to leverage the inherent reasoning capabilities of MLLMs to distinguish and predict object triplets across different depth layers. Beyond direct generation, other strategies involve optimizing instruction prompts to guide MLLM reasoning \cite{liu2025relation} or combining MLLM hidden representations with specialized projection heads to predict triplet scores \cite{jiang2025enhancing}. While these methods are powerful, they usually require huge computing resources \cite{li2025interaction}. On the other hand, \textbf{VLM-based} methods focus on contrastive pretraining to obtain a visual semantic space that can align visual and textual embeddings. These methods typically follow a two-stage framework: (1) pretraining a VLM model and (2) fine-tuning for relation prediction, referred to as Knowledge Infusion and Knowledge Transfer in some works \cite{li2025taking,li2025interaction}. He et al. \cite{he2022towards} introduce simple contrastive pretraining and predict relation via visual and textual alignment.
OvSGTR \cite{chen2024expanding} try to mitigate the catastrophic forgetting during fine-tuning by a visual-concept retention mechanism within a DETR-like architecture \cite{carion2020end}, utilizing knowledge distillation \cite{hinton2015distilling} to constrain feature drift from the pretrained VLM. More recently, interaction-centric approaches, which explicitly model interaction to distinguish meaningful triplets from background noise, have gained popularity. Several studies introduce bidirectional interaction mechanisms \cite{li2025taking,li2025interaction} or enhance VLM architectures by embedding visual features and adding additional encoder-decoder layers \cite{min2025interactive}. Specifically, the Interaction-Centric Model (ACC) \cite{li2025interaction} combines bidirectional interaction prompts with Grounding DINO \cite{liu2024grounding} to guide pretraining during the Infusion stage. This is followed with matching during the Transfer stage to produce more accurate scene graphs \cite{li2025taking,li2025interaction}.
Our paper focuses on VLM-based methods and utilize the two stage approach of Knowledge Infusion and Knowledge Transfer.

% \subsubsection{Panoptic Scene Graph Generation (PSG)}
% Panoptic Scene Graph Generation (PSG) extends conventional SGG by representing objects with segmentation masks rather than bounding boxes. Previous PSG research \cite{zhou2024openpsg} proposes a novel method that combines OpenSeeD \cite{zhang2023simple} for open-vocabulary object detection and segmentation, Transformer architectures \cite{vaswani2017attention} for object pairs filtering, and MLLMs (e.g., BLIP-2 \cite{li2023blip2}) for relation prediciton. Our paper aligns with this direction by incorporating GroundingDINO \cite{liu2024grounding} as a open-vocabulary object detectior, then SAM \cite{kirillov2023segment} for object segmentation.

% \subsubsection{Knowledge Distillation (KD)} It adapts teacher-student frameworks to transfer structured relational knowledge from large VLMs to more compact models. This enable zero-shot predicate prediction on unseen triples without exhaustive pairwise annotations. Pioneered by Hinton et al. \cite{hinton2015distilling}, early methods like response-based logit matching have evolved into feature-based alignment \cite{zagoruyko2017paying} and relation-preserving distillation \cite{kim2018paraphrasing}, with online variants such as Lan et al.'s \cite{lan2018knowledge} facilitating joint training on visual-linguistic graphs. Surveys by Gou et al. \cite{gou2021knowledge} and Beyer et al. \cite{beyer2022knowledge} highlight applications in Ov-SGG, where teachers like CLIP \cite{radford2021learning} or GLIP \cite{li2022grounded} provide dense triplet logits or embeddings to students, boosting long-tail and novel predicate detection. Even though critiques like Cho and Lee's \cite{cho2019efficacy} note challenges in aligning open-set semantics and mitigating teacher biases in compositional reasoning. Our approach integrates a point-to-point concept retention with the teacher model (CLIP's textual embedding) \cite{radford2021learning} to address information sparsity and closed-set limitations.

% \subsubsection{Closed-vocabulary SGG} is divided into one-stage and two-stage methods \cite{li2018factorizable},
% with the latter being more widely used \cite{dhingra2021bgt,lin2020gps,xu2017scene,yang2018graph,zellers2018neural}.
% The two-stage pipeline includes object detection first and then relation detection.
% Unlike two-stage approaches that rely on a fixed, pre-trained object detector to generate proposals,
% one-stage methods employ Transformer-based architectures \cite{cong2023reltr,li2022sgtr},
% and utilize learnable queries to simultaneously decode entities and their semantic relations,
% formulating SGG as a direct set prediction problem, enabling faster inference speed. Separately,
% Panoptic approaches \cite{hu2025spade,wang2024pair,yang2022panoptic} add pixel granularity
% to object detection by providing a unified segmentation of all image regions,
% combining instance segmentation for foreground objects with semantic segmentation for background regions,
% offering dense spatial understanding.

% \subsubsection{Object Detection.} Object detection plays a crucial role in the traditional two-stage SGG pipeline.
% Two-stage methods typically employ models such as Faster R-CNN \cite{ren2015faster} or YOLO \cite{redmon2015yolo},
% both of which rely on multi-stage processing that involves region proposals, anchor boxes, and non-maximum suppression.
% In contrast, DETR \cite{carion2020end} introduced an end-to-end transformer-based approach that formulates object detection as a direct set-prediction problem.
% This method uses a transformer encoder-decoder architecture Dwith self-attention, enabling global contextual reasoning across the entire image and naturally capturing relationships among objects.
% The architecture of DETR has inspired several follow-up works, including RelTR \cite{cong2023reltr} and EGTR \cite{im2024egtr},
% which adapt DETR's object queries and attention mechanisms to predict relations directly,
% thereby supporting a one-stage, end-to-end pipeline.


% Related techniques relevant to the chosen approach should be described in this section.
% Each technique should be presented in a separate paragraph, summarizing its role in the overall problem and acknowledging the common methods associated with it.
% For example, if your method incorporates data augmentation, you should briefly explain the purpose of augmentation and mention typical techniques such as CutMix or MixUp.
